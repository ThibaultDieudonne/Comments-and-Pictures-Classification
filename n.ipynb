{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "527d7230",
   "metadata": {},
   "source": [
    "# Améliorez le produit IA de votre start-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efd2e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libs\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import operator\n",
    "import warnings\n",
    "# text \n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# image\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# nltk.download()\n",
    "\n",
    "debug = 1\n",
    "\n",
    "if debug:\n",
    "    nrows = 1000\n",
    "else:\n",
    "    nrows = None\n",
    "    \n",
    "filename = './yelp_dataset/yelp_academic_dataset_review.json'\n",
    "\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e19882c",
   "metadata": {},
   "source": [
    "### Comments analysis Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c42b4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "stop_words = [stemmer.stem(w) for w in list(nltk.corpus.stopwords.words('english'))]\n",
    "\n",
    "def clean_up(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [stemmer.stem(w) for w in tokens]\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4afd7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_comments(comments):\n",
    "    negative_comments = comments[comments[\"stars\"]<3][\"text\"].to_list()\n",
    "\n",
    "    for comment_id in range(len(negative_comments)):\n",
    "        negative_comments[comment_id] = negative_comments[comment_id].replace('\\n', ' ')\n",
    "        \n",
    "    return negative_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ad8cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_json(filename, lines=True, nrows=nrows)\n",
    "\n",
    "negative_comments = get_negative_comments(comments)\n",
    "\n",
    "tokenized_text = [*map(clean_up, negative_comments)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1baaac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = {}\n",
    "for comment in tokenized_text:\n",
    "    for word in list(set(comment)):\n",
    "        if word in freqs:\n",
    "            freqs[word] += 1\n",
    "        else:\n",
    "            freqs[word] = 1\n",
    "\n",
    "freqs_list = [(x, freqs[x]/nrows)for x in freqs]\n",
    "freqs_list = sorted(freqs_list, key=lambda x:x[1])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "466e6e03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(freqs_list[:500])\n",
    "\n",
    "n = 200 # deleting the 200 most used words\n",
    "\n",
    "n_frequent = [freqs_list[x][0]for x in range(n)]\n",
    "\n",
    "def clear_words(tokens):\n",
    "    return [w for w in tokens if not w in n_frequent]\n",
    "\n",
    "tokenized_text = [*map(clear_words, tokenized_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd7bc0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [' '.join(comment) for comment in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8648707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "\n",
    "vectoriser = CountVectorizer(max_df=.6, min_df=10, max_features=100)\n",
    "vectorized = vectoriser.fit_transform(text)\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics, \n",
    "        max_iter=5, \n",
    "        learning_method='online', \n",
    "        learning_offset=50.,\n",
    "        random_state=42).fit(vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23092ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "hard fix drive yelp pick wrong sorri owner problem treat\n",
      "Topic 1:\n",
      "sandwich sauc bread appet plate dinner item chang bring arriv\n",
      "Topic 2:\n",
      "okay kid waiter almost bread past drive quick half sat\n",
      "Topic 3:\n",
      "poor door night total absolut understand probabl 20 decent may\n",
      "Topic 4:\n",
      "free cook enough clean kid home park wast select bread\n",
      "Topic 5:\n",
      "phone let decent later item clear store issu sat problem\n",
      "Topic 6:\n",
      "shop item wish move horribl store bring mean wrong month\n",
      "Topic 7:\n",
      "card clean credit wast care ago sat dinner send store\n",
      "Topic 8:\n",
      "care across overal point matter recent may yelp soon arriv\n",
      "Topic 9:\n",
      "employe dri notic item gave clear respons woman home matter\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, ntw): # do multiple runs and hyperparameters optimization\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-ntw-1:-1]]))\n",
    "\n",
    "n_top_words = 10\n",
    "display_topics(lda, vectoriser.get_feature_names(), n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0276376b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_names = vectoriser.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9fc5fe03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>absolut</th>\n",
       "      <th>across</th>\n",
       "      <th>ago</th>\n",
       "      <th>almost</th>\n",
       "      <th>appet</th>\n",
       "      <th>arriv</th>\n",
       "      <th>base</th>\n",
       "      <th>bit</th>\n",
       "      <th>bland</th>\n",
       "      <th>...</th>\n",
       "      <th>treat</th>\n",
       "      <th>understand</th>\n",
       "      <th>waiter</th>\n",
       "      <th>wast</th>\n",
       "      <th>wish</th>\n",
       "      <th>without</th>\n",
       "      <th>woman</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yelp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      20  absolut  across    ago  almost  appet  arriv   base    bit  bland  \\\n",
       "0  False    False   False  False   False  False  False  False  False  False   \n",
       "1  False    False   False  False   False  False  False  False  False  False   \n",
       "2  False    False   False  False   False  False  False  False  False  False   \n",
       "3  False    False   False  False   False  False  False  False  False  False   \n",
       "4  False    False   False  False   False  False  False  False  False  False   \n",
       "\n",
       "   ...  treat  understand  waiter   wast   wish  without  woman  write  wrong  \\\n",
       "0  ...  False       False   False  False  False    False  False  False  False   \n",
       "1  ...  False       False   False  False  False    False  False  False  False   \n",
       "2  ...  False       False   False  False  False    False  False  False  False   \n",
       "3  ...  False       False   False  False  False    False  False  False  False   \n",
       "4  ...  False       False   False  False  False     True   True  False  False   \n",
       "\n",
       "    yelp  \n",
       "0  False  \n",
       "1  False  \n",
       "2  False  \n",
       "3  False  \n",
       "4  False  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df = pd.DataFrame(columns=features_names, data=vectorized.toarray())\n",
    "for col_name in features_names:\n",
    "    words_df[col_name] = words_df[col_name].astype('bool')\n",
    "words_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f2b68e",
   "metadata": {},
   "source": [
    "### Picture analysis baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1705d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = None\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d45d5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(name):\n",
    "    return os.path.join(\"preprocessed_imgs\", f\"{name}.jpg\")\n",
    "\n",
    "def load_img(path, tgt):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.io.decode_jpeg(img)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.keras.applications.vgg16.preprocess_input(img)\n",
    "    tgt = tf.reshape(tgt, [1])\n",
    "    return img, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c65475a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>193365</th>\n",
       "      <td>preprocessed_imgs\\54sA7Q7qqD7DMv1y7Hflpg.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137348</th>\n",
       "      <td>preprocessed_imgs\\_WPfNzS5f3rzCiivf2gaVw.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148766</th>\n",
       "      <td>preprocessed_imgs\\zmXyOs99i_1K5eWZQXMDww.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100572</th>\n",
       "      <td>preprocessed_imgs\\lfqE42tg1a5i-Hbg2r9J6Q.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>preprocessed_imgs\\u0WsndgqhyzsvByprRatAQ.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  label\n",
       "193365  preprocessed_imgs\\54sA7Q7qqD7DMv1y7Hflpg.jpg      2\n",
       "137348  preprocessed_imgs\\_WPfNzS5f3rzCiivf2gaVw.jpg      1\n",
       "148766  preprocessed_imgs\\zmXyOs99i_1K5eWZQXMDww.jpg      1\n",
       "100572  preprocessed_imgs\\lfqE42tg1a5i-Hbg2r9J6Q.jpg      0\n",
       "1242    preprocessed_imgs\\u0WsndgqhyzsvByprRatAQ.jpg      4"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pictures = pd.read_json(\"photos.json\", lines=True, nrows=nrows)[[\"photo_id\", \"label\"]]\n",
    "pictures[\"path\"] = pictures[\"photo_id\"].apply(get_path)\n",
    "pictures = pictures[[\"path\", \"label\"]]\n",
    "pictures = pictures.sample(frac=1)\n",
    "pictures[\"label\"] = pictures[\"label\"].apply(lambda x:  {'interior': 0, 'outside': 1, 'menu': 2, 'food': 3, 'drink': 4}[x])\n",
    "\n",
    "pictures = pictures.head(100) # warning\n",
    "\n",
    "pictures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e8b2493",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "ds = tf.data.Dataset.from_tensor_slices((pictures[\"path\"].values, tf.cast(pictures[\"label\"].values, tf.int32))).shuffle(BUFFER_SIZE)\n",
    "# ds = tf.data.Dataset.from_tensor_slices((pictures[\"path\"].values, pictures[\"label\"].values)).shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "45b312dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(load_img, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b4c8a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_shapes(image, label):\n",
    "    image.set_shape((224, 224, 3))\n",
    "    return image, label\n",
    "\n",
    "ds = ds.map(set_shapes, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f94ca47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.repeat()\n",
    "ds = ds.batch(batch_size)\n",
    "ds = ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ba317bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_errors():\n",
    "    errors = 0\n",
    "    for image, label in ds.take(50):\n",
    "        if image.shape != (224, 224, 3):\n",
    "            errors += 1\n",
    "            print(image.shape)\n",
    "    if not errors:\n",
    "        print('All good')\n",
    "        \n",
    "# check_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b6f840b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " logits and labels must have the same first dimension, got logits shape [4900,5] and labels shape [100]\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at \\AppData\\Local\\Temp/ipykernel_9336/2769659046.py:9) ]] [Op:__inference_train_function_3332]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9336/2769659046.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9336/2769659046.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mnew_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmodel_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  logits and labels must have the same first dimension, got logits shape [4900,5] and labels shape [100]\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at \\AppData\\Local\\Temp/ipykernel_9336/2769659046.py:9) ]] [Op:__inference_train_function_3332]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    model = tf.keras.applications.vgg16.VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "    x = model.output\n",
    "    predictions = tf.keras.layers.Dense(5, activation='softmax')(x)\n",
    "    new_model = tf.keras.Model(inputs=model.input, outputs=predictions)\n",
    "    for layer in model.layers[:5]:\n",
    "        layer.trainable = False\n",
    "    new_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "    model_info = new_model.fit(ds, epochs=10, steps_per_epoch=1, verbose=2)\n",
    "    \n",
    "    return model_info\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c9d4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
