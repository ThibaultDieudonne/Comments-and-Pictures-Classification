{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "527d7230",
   "metadata": {},
   "source": [
    "# Am√©liorez le produit IA de votre start-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "efd2e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libs\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import operator\n",
    "import warnings\n",
    "# text \n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# image\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# nltk.download()\n",
    "\n",
    "debug = 1\n",
    "\n",
    "if debug:\n",
    "    nrows = 1000\n",
    "else:\n",
    "    nrows = None\n",
    "    \n",
    "filename = './yelp_dataset/yelp_academic_dataset_review.json'\n",
    "\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e19882c",
   "metadata": {},
   "source": [
    "### Comments analysis Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c42b4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "stop_words = [stemmer.stem(w) for w in list(nltk.corpus.stopwords.words('english'))]\n",
    "\n",
    "def clean_up(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [stemmer.stem(w) for w in tokens]\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4afd7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_comments(comments):\n",
    "    negative_comments = comments[comments[\"stars\"]<3][\"text\"].to_list()\n",
    "\n",
    "    for comment_id in range(len(negative_comments)):\n",
    "        negative_comments[comment_id] = negative_comments[comment_id].replace('\\n', ' ')\n",
    "        \n",
    "    return negative_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6ad8cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_json(filename, lines=True, nrows=nrows)\n",
    "\n",
    "negative_comments = get_negative_comments(comments)\n",
    "\n",
    "tokenized_text = [*map(clean_up, negative_comments)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1baaac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = {}\n",
    "for comment in tokenized_text:\n",
    "    for word in list(set(comment)):\n",
    "        if word in freqs:\n",
    "            freqs[word] += 1\n",
    "        else:\n",
    "            freqs[word] = 1\n",
    "\n",
    "freqs_list = [(x, freqs[x]/nrows)for x in freqs]\n",
    "freqs_list = sorted(freqs_list, key=lambda x:x[1])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "466e6e03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(freqs_list[:500])\n",
    "\n",
    "n = 200 # deleting the 200 most used words\n",
    "\n",
    "n_frequent = [freqs_list[x][0]for x in range(n)]\n",
    "\n",
    "def clear_words(tokens):\n",
    "    return [w for w in tokens if not w in n_frequent]\n",
    "\n",
    "tokenized_text = [*map(clear_words, tokenized_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bd7bc0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [' '.join(comment) for comment in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8648707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "\n",
    "vectoriser = CountVectorizer(max_df=.6, min_df=10, max_features=100)\n",
    "vectorized = vectoriser.fit_transform(text)\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "        n_components=n_topics, \n",
    "        max_iter=5, \n",
    "        learning_method='online', \n",
    "        learning_offset=50.,\n",
    "        random_state=42).fit(vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "23092ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "hard fix drive yelp pick wrong sorri owner problem treat\n",
      "Topic 1:\n",
      "sandwich sauc bread appet plate dinner item chang bring arriv\n",
      "Topic 2:\n",
      "okay kid waiter almost bread past drive quick half sat\n",
      "Topic 3:\n",
      "poor door night total absolut understand probabl 20 decent may\n",
      "Topic 4:\n",
      "free cook enough clean kid home park wast select bread\n",
      "Topic 5:\n",
      "phone let decent later item clear store issu sat problem\n",
      "Topic 6:\n",
      "shop item wish move horribl store bring mean wrong month\n",
      "Topic 7:\n",
      "card clean credit wast care ago sat dinner send store\n",
      "Topic 8:\n",
      "care across overal point matter recent may yelp soon arriv\n",
      "Topic 9:\n",
      "employe dri notic item gave clear respons woman home matter\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, ntw): # do multiple runs and hyperparameters optimization\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-ntw-1:-1]]))\n",
    "\n",
    "n_top_words = 10\n",
    "display_topics(lda, vectoriser.get_feature_names(), n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0276376b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_names = vectoriser.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9fc5fe03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>absolut</th>\n",
       "      <th>across</th>\n",
       "      <th>ago</th>\n",
       "      <th>almost</th>\n",
       "      <th>appet</th>\n",
       "      <th>arriv</th>\n",
       "      <th>base</th>\n",
       "      <th>bit</th>\n",
       "      <th>bland</th>\n",
       "      <th>...</th>\n",
       "      <th>treat</th>\n",
       "      <th>understand</th>\n",
       "      <th>waiter</th>\n",
       "      <th>wast</th>\n",
       "      <th>wish</th>\n",
       "      <th>without</th>\n",
       "      <th>woman</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yelp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      20  absolut  across    ago  almost  appet  arriv   base    bit  bland  \\\n",
       "0  False    False   False  False   False  False  False  False  False  False   \n",
       "1  False    False   False  False   False  False  False  False  False  False   \n",
       "2  False    False   False  False   False  False  False  False  False  False   \n",
       "3  False    False   False  False   False  False  False  False  False  False   \n",
       "4  False    False   False  False   False  False  False  False  False  False   \n",
       "\n",
       "   ...  treat  understand  waiter   wast   wish  without  woman  write  wrong  \\\n",
       "0  ...  False       False   False  False  False    False  False  False  False   \n",
       "1  ...  False       False   False  False  False    False  False  False  False   \n",
       "2  ...  False       False   False  False  False    False  False  False  False   \n",
       "3  ...  False       False   False  False  False    False  False  False  False   \n",
       "4  ...  False       False   False  False  False     True   True  False  False   \n",
       "\n",
       "    yelp  \n",
       "0  False  \n",
       "1  False  \n",
       "2  False  \n",
       "3  False  \n",
       "4  False  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df = pd.DataFrame(columns=features_names, data=vectorized.toarray())\n",
    "for col_name in features_names:\n",
    "    words_df[col_name] = words_df[col_name].astype('bool')\n",
    "words_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f2b68e",
   "metadata": {},
   "source": [
    "### Picture analysis baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1705d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = None\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d45d5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(name):\n",
    "    return os.path.join(\"preprocessed_imgs\", f\"{name}.jpg\")\n",
    "\n",
    "def load_img(path, tgt):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.io.decode_jpeg(img)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.keras.applications.vgg16.preprocess_input(img)\n",
    "    tgt = tf.reshape(tgt, [1])\n",
    "    return img, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4c65475a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31491</th>\n",
       "      <td>preprocessed_imgs\\2v_cm39v3XxuDP1KZuGZ7g.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51152</th>\n",
       "      <td>preprocessed_imgs\\D5F0EtAceOBw33tl02X-xQ.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12803</th>\n",
       "      <td>preprocessed_imgs\\6AgziOSwJkQdVBfP74jnLw.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56545</th>\n",
       "      <td>preprocessed_imgs\\fKst1Q9_K2ylWiDxEnrbFg.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169606</th>\n",
       "      <td>preprocessed_imgs\\qawnsLXjftwSCArE1vpT5w.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  label\n",
       "31491   preprocessed_imgs\\2v_cm39v3XxuDP1KZuGZ7g.jpg      4\n",
       "51152   preprocessed_imgs\\D5F0EtAceOBw33tl02X-xQ.jpg      3\n",
       "12803   preprocessed_imgs\\6AgziOSwJkQdVBfP74jnLw.jpg      4\n",
       "56545   preprocessed_imgs\\fKst1Q9_K2ylWiDxEnrbFg.jpg      3\n",
       "169606  preprocessed_imgs\\qawnsLXjftwSCArE1vpT5w.jpg      2"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pictures = pd.read_json(\"photos.json\", lines=True, nrows=nrows)[[\"photo_id\", \"label\"]]\n",
    "pictures[\"path\"] = pictures[\"photo_id\"].apply(get_path)\n",
    "pictures = pictures[[\"path\", \"label\"]]\n",
    "pictures = pictures.sample(frac=1)\n",
    "pictures[\"label\"] = pictures[\"label\"].apply(lambda x:  {'interior': 0, 'outside': 1, 'menu': 2, 'food': 3, 'drink': 4}[x])\n",
    "\n",
    "pictures = pictures.head(100) # warning\n",
    "\n",
    "pictures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5e8b2493",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "ds = tf.data.Dataset.from_tensor_slices((pictures[\"path\"].values, tf.cast(pictures[\"label\"].values, tf.int32))).shuffle(BUFFER_SIZE)\n",
    "# ds = tf.data.Dataset.from_tensor_slices((pictures[\"path\"].values, pictures[\"label\"].values)).shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "45b312dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(load_img, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c747672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_shapes(image, label):\n",
    "    image.set_shape((224, 224, 3))\n",
    "    return image, label\n",
    "\n",
    "ds = ds.map(set_shapes, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8b65a393",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.repeat()\n",
    "ds = ds.batch(batch_size)\n",
    "ds = ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ba317bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_errors():\n",
    "    errors = 0\n",
    "    for image, label in ds.take(50):\n",
    "        if image.shape != (224, 224, 3):\n",
    "            errors += 1\n",
    "            print(image.shape)\n",
    "    if not errors:\n",
    "        print('All good')\n",
    "        \n",
    "# check_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b6f840b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 - 211s - loss: 4.7761 - accuracy: 0.1700\n",
      "Epoch 2/10\n",
      "1/1 - 193s - loss: 7.8604 - accuracy: 0.3200\n",
      "Epoch 3/10\n",
      "1/1 - 187s - loss: 6.3962 - accuracy: 0.1800\n",
      "Epoch 4/10\n",
      "1/1 - 185s - loss: 6.9612 - accuracy: 0.2300\n",
      "Epoch 5/10\n",
      "1/1 - 189s - loss: 60.5077 - accuracy: 0.1800\n",
      "Epoch 6/10\n",
      "1/1 - 213s - loss: 30175.8691 - accuracy: 0.1700\n",
      "Epoch 7/10\n",
      "1/1 - 189s - loss: 220490031700115456.0000 - accuracy: 0.3200\n",
      "Epoch 8/10\n",
      "1/1 - 187s - loss: nan - accuracy: 0.1200\n",
      "Epoch 9/10\n",
      "1/1 - 185s - loss: nan - accuracy: 0.1200\n",
      "Epoch 10/10\n",
      "1/1 - 186s - loss: nan - accuracy: 0.1200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x191cfa96670>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model():\n",
    "    model = tf.keras.applications.vgg16.VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "    x = model.output\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    predictions = tf.keras.layers.Dense(5, activation='softmax')(x)\n",
    "    new_model = tf.keras.Model(inputs=model.input, outputs=predictions)\n",
    "    for layer in model.layers[:5]:\n",
    "        layer.trainable = False\n",
    "    new_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "    model_info = new_model.fit(ds, epochs=10, steps_per_epoch=1, verbose=2)\n",
    "    \n",
    "    return model_info\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c9d4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
