{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17274dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "import cv2\n",
    "\n",
    "nrows = None\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea421e",
   "metadata": {},
   "source": [
    "# Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61eb295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "stop_words = [stemmer.stem(w) for w in list(nltk.corpus.stopwords.words('english'))]\n",
    "\n",
    "def clean_up(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [stemmer.stem(w) for w in tokens]\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def get_negative_comments(comments):\n",
    "    negative_comments = comments[comments[\"stars\"]<3][\"text\"].to_list()\n",
    "\n",
    "    for comment_id in range(len(negative_comments)):\n",
    "        negative_comments[comment_id] = negative_comments[comment_id].replace('\\n', ' ')\n",
    "        \n",
    "    return negative_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b52592ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './yelp_dataset/yelp_academic_dataset_review.json'\n",
    "chunksize = 64\n",
    "\n",
    "def tokenize_comments():\n",
    "    c=0\n",
    "    for comments in pd.read_json(filename, lines=True, chunksize=chunksize):\n",
    "        negative_comments = get_negative_comments(comments)\n",
    "        tokenized_text = [*map(clean_up, negative_comments)]\n",
    "        res_df = pd.DataFrame(data=tokenized_text, columns=['tokenized_text'])\n",
    "        if c:\n",
    "            res_df.to_csv('preprocessed_text.csv', mode='a', index=False, header=False)\n",
    "        else:\n",
    "            res_df.to_csv('preprocessed_text.csv', mode='a', index=False)\n",
    "            c=1\n",
    "\n",
    "# tokenize_comments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fb6b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_filename = './preprocessed_text.csv'\n",
    "\n",
    "df = pd.read_csv(t_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f66b13e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_used():\n",
    "    freqs = {}\n",
    "    tokenized_text = df[\"tokenized_text\"].to_list()\n",
    "    for comment in tokenized_text:\n",
    "        if isinstance(comment, str):\n",
    "            comment = comment.split()\n",
    "            for word in list(set(comment)):\n",
    "                if word in freqs:\n",
    "                    freqs[word] += 1\n",
    "                else:\n",
    "                    freqs[word] = 1\n",
    "\n",
    "    freqs_list = [(x, freqs[x]/df.shape[0])for x in freqs]\n",
    "    freqs_list = sorted(freqs_list, key=lambda x:x[1])[::-1]\n",
    "\n",
    "    out = [x[0]for x in freqs_list[:500]]\n",
    "\n",
    "    with open(\"most_used.txt\", \"w+\") as f:\n",
    "        for w in out:\n",
    "            f.write(w)\n",
    "            f.write('\\n') \n",
    "            \n",
    "# get_most_used()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e1f0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frequent = []\n",
    "\n",
    "with open(\"most_used.txt\", \"r\") as f:\n",
    "    for l in f:\n",
    "        n_frequent.append(l[:-1])\n",
    "\n",
    "n_frequent = n_frequent[:100]\n",
    "\n",
    "def clear_words(tokens):\n",
    "    if isinstance(tokens, str):\n",
    "        return \" \".join([w for w in tokens.split() if not w in n_frequent])\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def del_most_freq():\n",
    "    global df\n",
    "    df = df.dropna()\n",
    "    df[\"tokenized_text\"] = df[\"tokenized_text\"].apply(clear_words)\n",
    "    df.to_csv('preprocessed_reviews.csv', index=False)\n",
    "    \n",
    "del_most_freq()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f37436",
   "metadata": {},
   "source": [
    "# Preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10c2dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hisEqulColor(img):\n",
    "    ycrcb=cv2.cvtColor(img,cv2.COLOR_BGR2YCR_CB)\n",
    "    channels=cv2.split(ycrcb)\n",
    "    cv2.equalizeHist(channels[0],channels[0])\n",
    "    cv2.merge(channels,ycrcb)\n",
    "    cv2.cvtColor(ycrcb,cv2.COLOR_YCR_CB2BGR,img)\n",
    "    return img\n",
    "\n",
    "def preprocess_imgs():\n",
    "    save_path = \"preprocessed_imgs\"\n",
    "    pictures = pd.read_json(\"photos.json\", lines=True, nrows=nrows)[[\"photo_id\"]]\n",
    "    done = 1\n",
    "    for index, row in pictures.iterrows():\n",
    "        img = cv2.imread(os.path.join(\"./yelp_photos\", f\"{row['photo_id']}.jpg\"), -1)\n",
    "        img = cv2.resize(img, (224, 224)) # resize to vgg16 input size\n",
    "        img = cv2.blur(img,(5, 5)) # apply gaussian blur\n",
    "        img = hisEqulColor(img) # equalize histogram\n",
    "        cv2.imwrite(os.path.join(save_path, f\"{row['photo_id']}.jpg\"), img) # save image\n",
    "        if not done % 1000:\n",
    "            print(f\"Done {done}/200 000\")\n",
    "        done += 1\n",
    "        \n",
    "# preprocess_imgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8e641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
