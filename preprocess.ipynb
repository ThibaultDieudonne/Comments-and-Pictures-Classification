{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17274dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "import cv2\n",
    "\n",
    "nrows = None\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea421e",
   "metadata": {},
   "source": [
    "# Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61eb295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "stop_words = [stemmer.stem(w) for w in list(nltk.corpus.stopwords.words('english'))]\n",
    "\n",
    "def clean_up(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [stemmer.stem(w) for w in tokens]\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def get_negative_comments(comments):\n",
    "    negative_comments = comments[comments[\"stars\"]<3][\"text\"].to_list()\n",
    "\n",
    "    for comment_id in range(len(negative_comments)):\n",
    "        negative_comments[comment_id] = negative_comments[comment_id].replace('\\n', ' ')\n",
    "        \n",
    "    return negative_comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b52592ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './yelp_dataset/yelp_academic_dataset_review.json'\n",
    "chunksize = 64\n",
    "\n",
    "def tokenize_comments():\n",
    "    c=0\n",
    "    for comments in pd.read_json(filename, lines=True, chunksize=chunksize):\n",
    "        negative_comments = get_negative_comments(comments)\n",
    "        tokenized_text = [*map(clean_up, negative_comments)]\n",
    "        res_df = pd.DataFrame(data=tokenized_text, columns=['tokenized_text'])\n",
    "        if c:\n",
    "            res_df.to_csv('preprocessed_text.csv', mode='a', index=False, header=False)\n",
    "        else:\n",
    "            res_df.to_csv('preprocessed_text.csv', mode='a', index=False)\n",
    "            c=1\n",
    "\n",
    "# tokenize_comments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fb6b740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stay mani marriott renaiss marriott huge disap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>place use cool chill place bunch neanderth bou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>set perfect adequ food come close dine chain l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>never seem get order correct servic crappi foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disappoint bolt follow fanni fabric close like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>decid take laptop mactron base friend recommen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1 would like make clear visit offic comment de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>awe awe awe servic rude hostess dismiss care b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>recommend place ate grandfath halfway eat meal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>total kind place let skew review much wow plac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       tokenized_text\n",
       "0   stay mani marriott renaiss marriott huge disap...\n",
       "1   place use cool chill place bunch neanderth bou...\n",
       "2   set perfect adequ food come close dine chain l...\n",
       "3   never seem get order correct servic crappi foo...\n",
       "4   disappoint bolt follow fanni fabric close like...\n",
       "..                                                ...\n",
       "95  decid take laptop mactron base friend recommen...\n",
       "96  1 would like make clear visit offic comment de...\n",
       "97  awe awe awe servic rude hostess dismiss care b...\n",
       "98  recommend place ate grandfath halfway eat meal...\n",
       "99  total kind place let skew review much wow plac...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_filename = './preprocessed_text.csv'\n",
    "\n",
    "# df = pd.read_csv(t_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f66b13e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stay mani marriott renaiss marriott huge disappoint front desk atrium nice starbuck site nice room run old flat screen expect renaiss got hotel via pricelin rate 75 night good deal price true renaiss', 'place use cool chill place bunch neanderth bouncer hop steroid act like whatev want mani better place davi squar glad visit busi sad burren worst place davi', 'set perfect adequ food come close dine chain like chili victoria station barbecu better surpris alway pick coupon linwood restaur com', 'never seem get order correct servic crappi food inconsist gone hill steadili last 6 9 month never go', 'disappoint bolt follow fanni fabric close like fanni select small fabric lean toward contemporari also small stock fabric order howev enough room display select like tri paw closet twice mani cloth rack meant handl woman work time nice help howev mother bought fabric problem employe help us calcul yardag without take repeat consider brought pillow form doubt exact larg mother abl make project work came pillow short time pillow made realiz short return bolt month time fabric sold plan reorder fortun make number larg throw pillow number want flexibl upholsteri job piec partial complet obvious unaccept advis feel free shop figur measur', 'wish could give zero star call taker rude technician incompet manag know custom servic overpr quot 230 went anoth locksmith actual got lock quot total 79 technician late twice could pick lock unusu said would drill lock ok fail bring charg batteri never offer come back charg batteri soon could offer send someon els said could howev come back around 10 hour later tabl usual need get lock need minut said charg 30 servic fee servic never receiv contact manag seem task said right charg 30 refus technician offer come back later doubt manag knew tabl payment still reimburs 30', 'great coffe pastri barista excel staff sooooo lost vacant', 'almost desol restaur dingi eviron star deciev lunch menu claim dish 550 3 saturday still charg 2 star congeal beef soup star friend dish come 30 minut got mine get soup came star ask water 5 time get right bill came star welp seem star whoopsi daisi want forgiv sinc waitress new even manag step screw excus mediocr food', 'first arriv coco key greet number staff member check process slight awkward staff appear new train happen time time harm foul foul part came enter room carpet soggi near bathroom entir room permeat musti smell someth along line aeu de someon flood toilet unpack done sucker opportun stick thing make work call front desk short thereaft housekeep appear spray bottl investig agre carpet need clean would happen follow day began spritz assum anti mold stuff ask clear see point mean sort timid communic realli need outgrow long stori short time hose carpet wall doorway spray bottl left cough choke fume forc spend night door open weak attempt ventil room leav heart fl may donat small portion lung tissu probabl ask die much aliv howev doubt ever return coco key', 'love ale hous time 1 star place servic 1 star manag give absolut zero respect game soccer come sinc open seen numer time understaff final tournament year past shown capac shame great establish great employe manag could sort place would 5 star']\n"
     ]
    }
   ],
   "source": [
    "def get_most_used():\n",
    "    freqs = {}\n",
    "    tokenized_text = df[\"tokenized_text\"].to_list()\n",
    "    for comment in tokenized_text:\n",
    "        if isinstance(comment, str):\n",
    "            comment = comment.split()\n",
    "            for word in list(set(comment)):\n",
    "                if word in freqs:\n",
    "                    freqs[word] += 1\n",
    "                else:\n",
    "                    freqs[word] = 1\n",
    "\n",
    "    freqs_list = [(x, freqs[x]/df.shape[0])for x in freqs]\n",
    "    freqs_list = sorted(freqs_list, key=lambda x:x[1])[::-1]\n",
    "\n",
    "    out = [x[0]for x in freqs_list[:500]]\n",
    "\n",
    "    with open(\"most_used.txt\", \"w+\") as f:\n",
    "        for w in out:\n",
    "            f.write(w)\n",
    "            f.write('\\n') \n",
    "            \n",
    "# get_most_used()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e1f0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frequent = []\n",
    "\n",
    "with open(\"most_used.txt\", \"r\") as f:\n",
    "    for l in f:\n",
    "        n_frequent.append(l[:-1])\n",
    "\n",
    "n_frequent = n_frequent[:250]\n",
    "\n",
    "def clear_words(tokens):\n",
    "    if isinstance(tokens, str):\n",
    "        return \" \".join([w for w in tokens.split() if not w in n_frequent])\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def del_most_freq():\n",
    "    df[\"tokenized_text\"] = df[\"tokenized_text\"].apply(clear_words)\n",
    "    df.to_csv('preprocessed_reviews.csv')\n",
    "    \n",
    "# del_most_freq()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f37436",
   "metadata": {},
   "source": [
    "# Preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eee5310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photo_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Un_Og6jfhazVn7CxszkKEw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BFE1AFOs27scnnfeBf99ZA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7t-C0r1JRdoVD9FS7M-N7Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rLnw0d-YYZvT9kR4y7h7_Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cv5M8MDw8a5NEWvw2AQ4nw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 photo_id\n",
       "0  Un_Og6jfhazVn7CxszkKEw\n",
       "1  BFE1AFOs27scnnfeBf99ZA\n",
       "2  7t-C0r1JRdoVD9FS7M-N7Q\n",
       "3  rLnw0d-YYZvT9kR4y7h7_Q\n",
       "4  Cv5M8MDw8a5NEWvw2AQ4nw"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = \"preprocessed_imgs\"\n",
    "\n",
    "pictures = pd.read_json(\"photos.json\", lines=True, nrows=nrows)[[\"photo_id\"]]\n",
    "\n",
    "pictures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10c2dc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1000/200 000\n",
      "Done 2000/200 000\n",
      "Done 3000/200 000\n",
      "Done 4000/200 000\n",
      "Done 5000/200 000\n",
      "Done 6000/200 000\n",
      "Done 7000/200 000\n",
      "Done 8000/200 000\n",
      "Done 9000/200 000\n",
      "Done 10000/200 000\n",
      "Done 11000/200 000\n",
      "Done 12000/200 000\n",
      "Done 13000/200 000\n",
      "Done 14000/200 000\n",
      "Done 15000/200 000\n",
      "Done 16000/200 000\n",
      "Done 17000/200 000\n",
      "Done 18000/200 000\n",
      "Done 19000/200 000\n",
      "Done 20000/200 000\n",
      "Done 21000/200 000\n",
      "Done 22000/200 000\n",
      "Done 23000/200 000\n",
      "Done 24000/200 000\n",
      "Done 25000/200 000\n",
      "Done 26000/200 000\n",
      "Done 27000/200 000\n",
      "Done 28000/200 000\n",
      "Done 29000/200 000\n",
      "Done 30000/200 000\n",
      "Done 31000/200 000\n",
      "Done 32000/200 000\n",
      "Done 33000/200 000\n",
      "Done 34000/200 000\n",
      "Done 35000/200 000\n",
      "Done 36000/200 000\n",
      "Done 37000/200 000\n",
      "Done 38000/200 000\n",
      "Done 39000/200 000\n",
      "Done 40000/200 000\n",
      "Done 41000/200 000\n",
      "Done 42000/200 000\n",
      "Done 43000/200 000\n",
      "Done 44000/200 000\n",
      "Done 45000/200 000\n",
      "Done 46000/200 000\n",
      "Done 47000/200 000\n",
      "Done 48000/200 000\n",
      "Done 49000/200 000\n",
      "Done 50000/200 000\n",
      "Done 51000/200 000\n",
      "Done 52000/200 000\n",
      "Done 53000/200 000\n",
      "Done 54000/200 000\n",
      "Done 55000/200 000\n",
      "Done 56000/200 000\n",
      "Done 57000/200 000\n",
      "Done 58000/200 000\n",
      "Done 59000/200 000\n",
      "Done 60000/200 000\n",
      "Done 61000/200 000\n",
      "Done 62000/200 000\n",
      "Done 63000/200 000\n",
      "Done 64000/200 000\n",
      "Done 65000/200 000\n",
      "Done 66000/200 000\n",
      "Done 67000/200 000\n",
      "Done 68000/200 000\n",
      "Done 69000/200 000\n",
      "Done 70000/200 000\n",
      "Done 71000/200 000\n",
      "Done 72000/200 000\n",
      "Done 73000/200 000\n",
      "Done 74000/200 000\n",
      "Done 75000/200 000\n",
      "Done 76000/200 000\n",
      "Done 77000/200 000\n",
      "Done 78000/200 000\n",
      "Done 79000/200 000\n",
      "Done 80000/200 000\n",
      "Done 81000/200 000\n",
      "Done 82000/200 000\n",
      "Done 83000/200 000\n",
      "Done 84000/200 000\n",
      "Done 85000/200 000\n",
      "Done 86000/200 000\n",
      "Done 87000/200 000\n",
      "Done 88000/200 000\n",
      "Done 89000/200 000\n",
      "Done 90000/200 000\n",
      "Done 91000/200 000\n",
      "Done 92000/200 000\n",
      "Done 93000/200 000\n",
      "Done 94000/200 000\n",
      "Done 95000/200 000\n",
      "Done 96000/200 000\n",
      "Done 97000/200 000\n",
      "Done 98000/200 000\n",
      "Done 99000/200 000\n",
      "Done 100000/200 000\n",
      "Done 101000/200 000\n",
      "Done 102000/200 000\n",
      "Done 103000/200 000\n",
      "Done 104000/200 000\n",
      "Done 105000/200 000\n",
      "Done 106000/200 000\n",
      "Done 107000/200 000\n",
      "Done 108000/200 000\n",
      "Done 109000/200 000\n",
      "Done 110000/200 000\n",
      "Done 111000/200 000\n",
      "Done 112000/200 000\n",
      "Done 113000/200 000\n",
      "Done 114000/200 000\n",
      "Done 115000/200 000\n",
      "Done 116000/200 000\n",
      "Done 117000/200 000\n",
      "Done 118000/200 000\n",
      "Done 119000/200 000\n",
      "Done 120000/200 000\n",
      "Done 121000/200 000\n",
      "Done 122000/200 000\n",
      "Done 123000/200 000\n",
      "Done 124000/200 000\n",
      "Done 125000/200 000\n",
      "Done 126000/200 000\n",
      "Done 127000/200 000\n",
      "Done 128000/200 000\n",
      "Done 129000/200 000\n",
      "Done 130000/200 000\n",
      "Done 131000/200 000\n",
      "Done 132000/200 000\n",
      "Done 133000/200 000\n",
      "Done 134000/200 000\n",
      "Done 135000/200 000\n",
      "Done 136000/200 000\n",
      "Done 137000/200 000\n",
      "Done 138000/200 000\n",
      "Done 139000/200 000\n",
      "Done 140000/200 000\n",
      "Done 141000/200 000\n",
      "Done 142000/200 000\n",
      "Done 143000/200 000\n",
      "Done 144000/200 000\n",
      "Done 145000/200 000\n",
      "Done 146000/200 000\n",
      "Done 147000/200 000\n",
      "Done 148000/200 000\n",
      "Done 149000/200 000\n",
      "Done 150000/200 000\n",
      "Done 151000/200 000\n",
      "Done 152000/200 000\n",
      "Done 153000/200 000\n",
      "Done 154000/200 000\n",
      "Done 155000/200 000\n",
      "Done 156000/200 000\n",
      "Done 157000/200 000\n",
      "Done 158000/200 000\n",
      "Done 159000/200 000\n",
      "Done 160000/200 000\n",
      "Done 161000/200 000\n",
      "Done 162000/200 000\n",
      "Done 163000/200 000\n",
      "Done 164000/200 000\n",
      "Done 165000/200 000\n",
      "Done 166000/200 000\n",
      "Done 167000/200 000\n",
      "Done 168000/200 000\n",
      "Done 169000/200 000\n",
      "Done 170000/200 000\n",
      "Done 171000/200 000\n",
      "Done 172000/200 000\n",
      "Done 173000/200 000\n",
      "Done 174000/200 000\n",
      "Done 175000/200 000\n",
      "Done 176000/200 000\n",
      "Done 177000/200 000\n",
      "Done 178000/200 000\n",
      "Done 179000/200 000\n",
      "Done 180000/200 000\n",
      "Done 181000/200 000\n",
      "Done 182000/200 000\n",
      "Done 183000/200 000\n",
      "Done 184000/200 000\n",
      "Done 185000/200 000\n",
      "Done 186000/200 000\n",
      "Done 187000/200 000\n",
      "Done 188000/200 000\n",
      "Done 189000/200 000\n",
      "Done 190000/200 000\n",
      "Done 191000/200 000\n",
      "Done 192000/200 000\n",
      "Done 193000/200 000\n",
      "Done 194000/200 000\n",
      "Done 195000/200 000\n",
      "Done 196000/200 000\n",
      "Done 197000/200 000\n",
      "Done 198000/200 000\n",
      "Done 199000/200 000\n",
      "Done 200000/200 000\n"
     ]
    }
   ],
   "source": [
    "def hisEqulColor(img):\n",
    "    ycrcb=cv2.cvtColor(img,cv2.COLOR_BGR2YCR_CB)\n",
    "    channels=cv2.split(ycrcb)\n",
    "    cv2.equalizeHist(channels[0],channels[0])\n",
    "    cv2.merge(channels,ycrcb)\n",
    "    cv2.cvtColor(ycrcb,cv2.COLOR_YCR_CB2BGR,img)\n",
    "    return img\n",
    "\n",
    "def preprocess_imgs():\n",
    "    done = 1\n",
    "    for index, row in pictures.iterrows():\n",
    "        img = cv2.imread(os.path.join(\"./yelp_photos\", f\"{row['photo_id']}.jpg\"), -1)\n",
    "        img = cv2.resize(img, (224, 224)) # resize to vgg16 input size\n",
    "        img = cv2.blur(img,(5, 5)) # apply gaussian blur\n",
    "        img = hisEqulColor(img) # equalize histogram\n",
    "        cv2.imwrite(os.path.join(save_path, f\"{row['photo_id']}.jpg\"), img) # save image\n",
    "        if not done % 1000:\n",
    "            print(f\"Done {done}/200 000\")\n",
    "        done += 1\n",
    "        \n",
    "# preprocess_imgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8e641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
